{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiZWqqXbfP9/NrUbllzbB4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YusolCho/NLPstudy/blob/main/NLP_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_eE_NFR2wOfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **패키지 불러오기**"
      ],
      "metadata": {
        "id": "B300b593wPT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-oUhGT3Eu7lq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **데이터 불러오기**"
      ],
      "metadata": {
        "id": "S1KgM-2ZwWX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "review_df = pd.read_csv(\"/content/drive/My Drive/labeledTrainData.tsv\", sep='\\t', quoting=3)\n",
        "review_df.head(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "uUH0qAHdwb28",
        "outputId": "962b8f70-574b-4fe1-f86e-369f4dcbdcc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id  sentiment                                             review\n",
              "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
              "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
              "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a52ffa6-350a-4aa5-a501-12026703f0aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a52ffa6-350a-4aa5-a501-12026703f0aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6a52ffa6-350a-4aa5-a501-12026703f0aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6a52ffa6-350a-4aa5-a501-12026703f0aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. id : 각 데이터의 id\n",
        "2. sentiment : 영화평(review)의 Sentiment 결과 값(Target label)\n",
        "* 1은 긍정적 평가\n",
        "* 0은 부정적 평가\n",
        "3. review : 영화평의 텍스트\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Fdtrt0tyntr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(review_df[\"review\"][0]) #데이터의 첫번째 행에 해당되는 영화평의 텍스트 내용"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyCK8WHXyPmC",
        "outputId": "1b82d0d7-290e-4bc7-9f02-3e6de36638ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **노이즈 제거**\n",
        "* HTML 형식에서 추출하여 태그 존재 -> 모두 공백으로 변경\n",
        "* 영어가 아닌 특수문자/숫자 -> 모두 공백으로 변경(숫자도 Sentiment 를 위한 피처로 무의미!)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Oq3vbLwfzcIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# replace 함수 : <br> html 태그를 공백으로\n",
        "review_df[\"review\"] = review_df[\"review\"].str.replace(\"<br />\", \" \")\n",
        "\n",
        "# 정규표현식 모듈 re : 영어 문자열이 아닌 문자는 모두 공백으로 \n",
        "review_df[\"review\"] = review_df[\"review\"].apply(lambda x : re.sub(\"[^a-zA-Z]\", \" \", x))\n"
      ],
      "metadata": {
        "id": "zgoZGPFWz_xj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **피처 데이터 셋 생성**"
      ],
      "metadata": {
        "id": "c7qyymt908Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_df = review_df[\"sentiment\"]#sentiment(target변수)를 별도의 데이터셋으로 추출한 뒤\n",
        "feature_df = review_df.drop([\"id\",\"sentiment\"], axis = 1, inplace = False)#원본 데이터 셋에서 id와 sentiment 칼럼을 삭제\n",
        "\n",
        "# ****이걸 왜하는지..?"
      ],
      "metadata": {
        "id": "kied1jVq1AFt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **데이터셋 분리(학습용, 테스트용)**"
      ],
      "metadata": {
        "id": "vpFIWfn72EKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_df, class_df, test_size = 0.3, random_state = 156)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_GJzC951brb",
        "outputId": "8eb084c2-e518-4398-f8c6-b5e3b6876b40"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17500, 1), (7500, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **벡터화하고 모델 돌리기 (1)ML(Logistic Regression) without TF-IDF**"
      ],
      "metadata": {
        "id": "wpxZ4OIbWBFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 패키지 불러오기"
      ],
      "metadata": {
        "id": "GnyZ32_zcreQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # CountVectorizer:빈도수에 따른 벡터화 TfidfVectorizer:TF-IDF. \n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.linear_model import LogisticRegression #Logistic Regression Model 구현 위함\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score #모델 평가 : accuracy, roc-curve, auc 구하기 위함 "
      ],
      "metadata": {
        "id": "jcovXx3HWTIP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 여기서 sklearn.pipeline이란?\n",
        "* -> 파이프라인은 데이터 전처리에서 모델학습까지의 과정을 하나로 연결해주는 것이라고 보면 된다.\n",
        "* -> 파이프라인의 목적 : cross-validated될 수 있는 몇개의 단계를 다른 parameter 세팅이 가능하도록 조합하는 것. 이것을 위해 파이프라인에서 사용된 단계들 내에서 anova__k=10(anova의 parameter k의 값은 10)이런 식으로 \n",
        "'이름 __파라미터=파라미터값' 라는 문법을 통해 파라미터 값 설정 가능하도록 한다.  \n",
        "* https://zephyrus1111.tistory.com/254 여기 설명 참고 \n",
        "\n",
        "* Cross-Validation(CV) 의 뜻\n",
        "* -> 얘를 들어 가장 많이 사용되는 k-fold CV의 경우, training set를 k개로 쪼갠 다음 k-1개를 통해 모델을 학습시킨 뒤, 그 과정에서 찾은 최적의 parameter를 최종 parameter 로 채택하고 나머지 1개를 validation data로 사용하는 방법. 즉, 최적의 parameter를 찾기위한 기법 중 하나이다. \n"
      ],
      "metadata": {
        "id": "t6xyV9JBXI0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EZEKTY07cwC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파이프라인 구축 : 불용어 제거, 임베딩(using CountVectorization), 모델 구축"
      ],
      "metadata": {
        "id": "Gs3vVkbscvIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 스톱 워드는 English, filtering, ngram은 (1,2)로 설정해 CountVectorizer 수행\n",
        "# LogisticRegression의 C는 10으로 설정\n",
        "pipeline = Pipeline([ # sklearn의 pipeline 구축. []로 묶여있는 건 steps로, 튜플형태\n",
        "    (\"cnt_vect\", CountVectorizer(stop_words=\"english\", ngram_range=(1,2))), #steps의 첫번째 튜플. Pipeline 의 첫 단계명은 cnt_vect\n",
        "    (\"lr_clf\", LogisticRegression(C = 10,max_iter = 4000))]) #steps의 두번째 튜플. Pipeline 의 두번째 단계명은 lr_clf"
      ],
      "metadata": {
        "id": "11iZizzCa_Ep"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 fitting"
      ],
      "metadata": {
        "id": "tPcT1DrIfglH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline 객체를 이용하여 fit(), predict()로 학습/예측 수행\n",
        "# predict_proba()는 roc_auc 때문에 수행\n",
        "pipeline.fit(X_train[\"review\"], y_train)\n",
        "pred = pipeline.predict(X_test[\"review\"])\n",
        "pred_probs = pipeline.predict_proba(X_test[\"review\"])[:,1]\n",
        "\n",
        "print(\"예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}\".format(accuracy_score(y_test, pred), roc_auc_score(y_test, pred_probs)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBVn2Nk_fgKR",
        "outputId": "42bf36e3-5dee-4a39-8ef0-903516b22b9f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 정확도는 0.8860, ROC-AUC는 0.9503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **벡터화하고 모델 돌리기 (2)ML(Logistic Regression) with TF-IDF**"
      ],
      "metadata": {
        "id": "E-0eFHf1nfjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파이프라인 구축 : 불용어 제거, 임베딩(using TfidfVectorization), 모델 구축"
      ],
      "metadata": {
        "id": "dIgpt4Cpn9eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 스톱 워드는 English, filtering, ngram은 (1,2)로 설정해 CountVectorizer 수행\n",
        "# LogisticRegression의 C는 10으로 설정\n",
        "pipeline = Pipeline([\n",
        "    (\"cnt_vect\", TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))),\n",
        "    (\"lr_clf\", LogisticRegression(C = 10))])"
      ],
      "metadata": {
        "id": "OMBLlc9BneS1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 fitting"
      ],
      "metadata": {
        "id": "SN8xkkKjpOsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipeline.score(X_train[\"review\"],y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUKpr81FphtY",
        "outputId": "0101fe67-96d1-4ef7-f7c6-54c1d7c574a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9998285714285714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline 객체를 이용하여 fit(), predict()로 학습/예측 수행\n",
        "# predict_proba()는 roc_auc 때문에 수행\n",
        "pipeline.fit(X_train[\"review\"], y_train)\n",
        "pred = pipeline.predict(X_test[\"review\"])\n",
        "pred_probs = pipeline.predict_proba(X_test[\"review\"])[:,1]\n",
        "\n",
        "print(\"예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}\".format(accuracy_score(y_test, pred), roc_auc_score(y_test, pred_probs)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcH2Gwm4nqI3",
        "outputId": "ceb19f58-0ca2-4507-d928-92d537b77a82"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 정확도는 0.8936, ROC-AUC는 0.9598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **벡터화하고 모델 돌리기 (3)ML > Lexicon(비지도학습) > SentiWordNet**\n",
        "\n",
        "* Lexicon?\n",
        "* 일종의 감성 어휘 사전이라고 생각하면 됨 \n",
        "* 감성 분석을 위한 용어, 문맥에 대한 다양한 정보 보유 -> 문서의 긍부정 판단\n",
        "\n",
        "\n",
        "[SentiWordNet을 이용한 감성 분석]\n",
        "\n",
        "1. 문장 토큰화\n",
        "2. 다시 문장을 단어토큰화 + 품사 태깅\n",
        "3. 품사 태깅된 단어 기반으로 synset 객체와 senti_synset 객체를 생성\n",
        "4. Senti_synset에서 긍정 감성 / 부정 감성 지수를 구하고 이를 모두 합산하여 특정 임계치 값 이상일 때 긍정 감성으로, 그렇지 않을 때는 부정 감성으로 결정\n",
        "\n",
        "* 여기서 문득 궁금한 점 : 품사태깅 해주는 이유?\n",
        "* 품사태깅은 POS(Part of Speech) Tagging으로, 텍스트를 형태소 단위로 쪼개준 뒤 해당 형태소의 품사를 태깅하는 것을 말함 \n",
        "* 품사태깅의 활용 : 품사 태깅을 적용한 후, 명사만 추출하거나 주요 품사만 추출해 데이터로 사용할 수 있다. 예를 들어 영화 리뷰의 긍/부정을 파악하는데 '이다'라는 단어는 분류에 직접적으로 영향을 미치지 않을 것이라는 가정하에 주요 품사만 추출해 데이터로 사용한다.\n"
      ],
      "metadata": {
        "id": "xZ0k_reUsts3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 패키지 임포트 "
      ],
      "metadata": {
        "id": "oTFL7TpWvYkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사 태깅하는 내부 함수\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "# ****************wordnet이 뭔지 잘 모름. "
      ],
      "metadata": {
        "id": "ZoBouQnZswwJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 품사 태깅"
      ],
      "metadata": {
        "id": "Lsc_0-9vvlcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 간단한 NLTK PennTreebank Tag를 기반으로 WordNet 기반의 품사 Tag로 변환\n",
        "def penn_to_wn(tag):\n",
        "  if tag.startswith(\"J\"):\n",
        "    return wn.ADJ\n",
        "  elif tag.startswith(\"N\"):\n",
        "    return wn.NOUN\n",
        "  elif tag.startswith(\"R\"):\n",
        "    return wn.ADV\n",
        "  elif tag.startswith(\"V\"):\n",
        "    return wn.VERB"
      ],
      "metadata": {
        "id": "qXy78s6ovlxy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 패키지 추가 임포트"
      ],
      "metadata": {
        "id": "edYXVhiK3v4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n"
      ],
      "metadata": {
        "id": "qvxssXqOxNkr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 감성지수 구하는 함수 정의하기...?\n",
        "******어려워..."
      ],
      "metadata": {
        "id": "3coIUeH74HAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def swn_polarity(text):\n",
        "  # 감성 지수 초기화\n",
        "  sentiment = 0.0\n",
        "  tokens_count = 0\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  raw_sentences = sent_tokenize(text)\n",
        "  # 분해된 문장별로 단어 토근 --> 품사 태깅 후에 SentiSynset 생성 --> 감성 지수 합산\n",
        "  for raw_sentences in raw_sentences :\n",
        "    # NLTK 기반의 품사 태깅 문장 추출\n",
        "    tagged_sentence = pos_tag(word_tokenize(raw_sentences))\n",
        "    for word, tag in tagged_sentence:\n",
        "\n",
        "      # WordNet 기반 품사 태깅과 어근 추출\n",
        "      wn_tag = penn_to_wn(tag)\n",
        "      if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "        continue\n",
        "      lemma = lemmatizer.lemmatize(word, pos = wn_tag)\n",
        "      \n",
        "      if not lemma:\n",
        "        continue\n",
        "      \n",
        "      # 어근을 추출한 단어와 WordNet 기반 품사 태깅을 입력해 Synset 객체 생성\n",
        "      synsets = wn.synsets(lemma, pos = wn_tag)\n",
        "      if not synsets:\n",
        "        continue\n",
        "      \n",
        "      # sentiwordnet의 감성 단어 분석으로 감성 synset 추출\n",
        "      # 모든 단어에 대해 긍정 감성 지수는 +로, 부정 감성 지수는 -로 합산하여 감성 지수 계산\n",
        "      synset = synsets[0]\n",
        "      swn_synset = swn.senti_synset(synset.name())\n",
        "      sentiment += (swn_synset.pos_score() - swn_synset.neg_score())\n",
        "      tokens_count += 1\n",
        "\n",
        "  if not tokens_count :\n",
        "    return 0\n",
        "\n",
        "  # 총 score가 0 이상일 경우 긍정(Positive) 1, 그렇지 않을 경우 부정(Negative) 0 반환\n",
        "  if sentiment >= 0:\n",
        "    return 1\n",
        "\n",
        "  return 0"
      ],
      "metadata": {
        "id": "Ew7o9wAexObm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ? 뭐시기 다운로드"
      ],
      "metadata": {
        "id": "qc_yxjKx4SWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('sentiwordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdzWz0VZxOir",
        "outputId": "8eaf04a0-a98b-481c-acf1-ec32831a0fee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_df[\"preds\"] = review_df[\"review\"].apply( lambda x : swn_polarity(x))\n",
        "y_target = review_df[\"sentiment\"].values\n",
        "preds = review_df[\"preds\"].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "DW9xMfsXxOpy",
        "outputId": "ed4a8c0a-75ea-4c87-d332-29dc35174f75"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ef7d5beaf2e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreview_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preds\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mswn_polarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ef7d5beaf2e6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreview_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preds\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mswn_polarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-16dff084ed3b>\u001b[0m in \u001b[0;36mswn_polarity\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mraw_sentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_sentences\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# NLTK 기반의 품사 태깅 문장 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtagged_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_sentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m    165\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 성능 평가 (분류 성능평가)"
      ],
      "metadata": {
        "id": "Hr33jixl2-0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "def get_clf_eval(y_test, pred):\n",
        "  confusion = confusion_matrix(y_test, pred)\n",
        "  accuracy = accuracy_score(y_test, pred)\n",
        "  precision = precision_score(y_test, pred)\n",
        "  recall = recall_score(y_test, pred)\n",
        "  print(\"오차행렬\")\n",
        "  print(confusion)\n",
        "  print(\"정확도 : {0:.4f}, 정밀도 : {0:.4f}, 재현율 : {0:.4f}\". format(accuracy, precision, recall))"
      ],
      "metadata": {
        "id": "8_0TYNz8xOxC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" #### SentiWordNet 예측 성능 평가\")\n",
        "get_clf_eval(y_target, preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed3cXoWqxO3o",
        "outputId": "ea00a949-74c1-4ead-f8e2-a97fc98f67ce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " #### SentiWordNet 예측 성능 평가\n",
            "오차행렬\n",
            "[[7668 4832]\n",
            " [3636 8864]]\n",
            "정확도 : 0.6613, 정밀도 : 0.6613, 재현율 : 0.6613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **벡터화하고 모델 돌리기 (4)ML > Lexicon(비지도학습) > VADER Lexicon**\n",
        "* VADER는 소셜 미디어의 감성 분석 용도로 만들어진 룰 기반의 Lexicon\n",
        "\n",
        "[Vader를 이용한 감성 분석]\n",
        "\n",
        "(1)먼저 SentimentIntensityAnalyzer 객체를 생성\n",
        "\n",
        "(2)문서별로 polarity_scores() 메서드를 호출해 감성 점수를 구하기\n",
        "\n",
        "(3)해당 문서의 감성 점수가 특정 임계값 이상이면 긍정, 그렇지 않으면 부정으로 판단\n",
        "\n",
        "SentimentIntensityAnalyzer 객체의 polarity_scores() 메서드\n",
        ": 딕셔너리 형태의 감성 점수를 반환.\n",
        "\n",
        "* \"neg\" : 부정 감성 지수\n",
        "* \"neu\" : 중립적인 감성 지수\n",
        "* \"pos\" : 긍정 감성 지수\n",
        "* compound : 감성 지수. neg, neu, pos score를 적절히 조합해 -1에서 1 사이의 값을 가진다. 이 compound score를 기반으로 부정 감성 또는 긍정 감성 여부를 결정\n",
        "* 임계값 : 보통 0.1 이상이면 긍정 감성, 그 이하이면 부정 감성으로 판단(케바케. 임계값 조절가능)"
      ],
      "metadata": {
        "id": "ZtmBIL894Z3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 뭐시기 다운로드, 패키지 임포트"
      ],
      "metadata": {
        "id": "7sSVcgzZ4ryV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUL_KNzAxO-P",
        "outputId": "ad4f026b-9c31-4ea3-c1f5-1ffb3bb86651"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SentimentIntensityAnalyzer 객체를 생성"
      ],
      "metadata": {
        "id": "3-Hnc34c4yIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "senti_analyzer = SentimentIntensityAnalyzer()\n",
        "senti_scores = senti_analyzer.polarity_scores(review_df[\"review\"][0]) #polarity_scores() 메서드 : 딕셔너리 형태의 감성 점수를 반환.\n",
        "print(senti_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCQGvausxPFZ",
        "outputId": "2153715d-a896-4bfb-8bf2-8566b6152048"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.13, 'neu': 0.743, 'pos': 0.127, 'compound': -0.7943}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이 과정이 왜 필요한건지 모르겠음 그냥 위에 함수처럼 해주면 되는거 아닌가 싶고.. \n"
      ],
      "metadata": {
        "id": "70mLHv4K9DyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vader_polarity(review, threshold = 0.1): #threshold로 임계값을 설정한다. \n",
        "  analyzer = SentimentIntensityAnalyzer() #객체 생성\n",
        "  scores = analyzer.polarity_scores(review) #SentimentIntensityAnalyzer()의 객체의 polarity_scores() 메서드 : 딕셔너리 형태의 감성 점수를 반환.\n",
        "\n",
        "  # compound 값에 기반해 threshold 입력값보다 크면 1, 그렇지 않으면 0을 반환\n",
        "  agg_score = scores[\"compound\"] #감성 점수가 할당되어있는 scores 변수 내의 compound 값을 추출\n",
        "  final_sentiment = 1 if agg_score >= threshold else 0 #그 compound 값을 threshold 값과 비교하여 0또는 1 할당\n",
        "  return final_sentiment"
      ],
      "metadata": {
        "id": "K7IfkSrcxPTS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## "
      ],
      "metadata": {
        "id": "HIGnFfX99YWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply lambda 식을 이용해 레코드별로 vader_polarity를 수행하고 결과를 \"vader_preds\"에 저장\n",
        "review_df[\"vader_preds\"] = review_df[\"review\"].apply( lambda x : vader_polarity(x, 0.1))\n",
        "#문서에 새로운 열을 추가하고, 문서의 각 행에 대해 위의 vader_polarity함수를 적용한 결과값을 그 열에 담는다 \n",
        "y_target = review_df[\"sentiment\"].values #실제값\n",
        "vader_preds = review_df[\"vader_preds\"].values #예측값 \n",
        "\n",
        "print(\"#### VADER 예측 성능 평가 ####\")\n",
        "get_clf_eval(y_target, vader_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXohXrjbxPYC",
        "outputId": "2f008673-3237-4cc4-db99-63f64f0392a9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### VADER 예측 성능 평가 ####\n",
            "오차행렬\n",
            "[[ 6747  5753]\n",
            " [ 1858 10642]]\n",
            "정확도 : 0.6956, 정밀도 : 0.6956, 재현율 : 0.6956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* *********apply(lambda x: 이거 처음보는건데 뭐임"
      ],
      "metadata": {
        "id": "4X_GTgnG-6Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTEJndsrxPeh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}